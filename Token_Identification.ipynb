{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmoghTantradi/CS182-final-project/blob/main/Token_Identification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NIBMvdZLmCb"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import gensim.downloader as api\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "model = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "\n",
        "\n",
        "def get_rarities(tokenizer):\n",
        "    # Download the Gutenberg Corpus (if not already downloaded)\n",
        "    nltk.download('gutenberg')\n",
        "    # Download the Punkt tokenizer models\n",
        "    nltk.download('punkt')\n",
        "    # Load the Gutenberg Corpus\n",
        "    corpus = gutenberg.raw('bible-kjv.txt')  # You can choose a specific Gutenberg text\n",
        "\n",
        "    # Tokenize the entire corpus\n",
        "    tokens = word_tokenize(corpus)\n",
        "\n",
        "    # Calculate token frequencies\n",
        "    token_frequencies = Counter(tokens)\n",
        "\n",
        "    # Calculate relative rarity for each token\n",
        "    total_tokens = len(tokens)\n",
        "    relative_rarity = {token: freq / total_tokens for token, freq in token_frequencies.items()}\n",
        "\n",
        "    # Sort tokens by rarity (from rarest to most common)\n",
        "    sorted_tokens_by_rarity = sorted(relative_rarity.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return sorted_tokens_by_rarity\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similarities(tokens, category):\n",
        "  similarities = []\n",
        "  if category not in model:\n",
        "    print(\"ERROR: THE CATEGORY IS NOT IN THE VOCABULARY\")\n",
        "    return\n",
        "  for token_pair in tokens:\n",
        "    token = token_pair[0]\n",
        "\n",
        "\n",
        "    # Calculate the cosine similarity between the word vectors\n",
        "    if token not in model:\n",
        "      similarities.append((token, 1)) #If the word is not present, we will not choose it, so we set maximum cosine similarity\n",
        "    else:\n",
        "\n",
        "      cosine_sim = cosine_similarity([model[token]], [model[category]])\n",
        "\n",
        "      similarities.append((token, 1+cosine_sim.item()))\n",
        "  #normalize similarities:\n",
        "  total_sim = 0\n",
        "  for i in range(len(similarities)):\n",
        "    total_sim += similarities[i][1]\n",
        "  for i in range(len(similarities)):\n",
        "    similarities[i] = (similarities[i][0], similarities[i][1]/total_sim)\n",
        "\n",
        "  #sort by similarity:\n",
        "\n",
        "  return similarities\n"
      ],
      "metadata": {
        "id": "hFOnoXbaLo-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def composite_token_ranking(token_rarities, token_similarities):\n",
        "  composite_ranking = []\n",
        "  n = len(token_rarities)\n",
        "  for i in range(n):\n",
        "    token = token_rarities[i][0]\n",
        "    rarity = token_rarities[i][1]\n",
        "    similarity = token_similarities[i][1]\n",
        "    composite_ranking.append((token, rarity + similarity + ((rarity*similarity)**2)))\n",
        "  return composite_ranking\n"
      ],
      "metadata": {
        "id": "rDfFOqlrLtXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_tokens(category, num_tokens):\n",
        "  token_rarities = get_rarities(tokenizer)\n",
        "  token_similarities = get_similarities(token_rarities, category)\n",
        "  composite_ranking = composite_token_ranking(token_rarities, token_similarities)\n",
        "  composite_ranking = sorted(composite_ranking, key=lambda x: x[1])\n",
        "  return composite_ranking[:num_tokens]"
      ],
      "metadata": {
        "id": "jFmMAK-nLwBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_tokens_two_categories(category1, category2, num_tokens):\n",
        "  tokens1 = find_tokens(category1, num_tokens)\n",
        "  tokens2 = find_tokens(category2, num_tokens)\n",
        "  two_token_ranking = []\n",
        "  n = len(tokens1)\n",
        "  for i in range(n):\n",
        "    for j in range(n):\n",
        "      token1 = tokens1[i][0]\n",
        "      token2 = tokens2[j][0]\n",
        "      cosine_sim = cosine_similarity([model[token1]], [model[token2]]).item()\n",
        "      two_token_ranking.append((token1, token2, cosine_sim))\n",
        "  two_token_ranking = sorted(two_token_ranking, key=lambda x: x[2])\n",
        "  return two_token_ranking[:num_tokens]"
      ],
      "metadata": {
        "id": "3iAPvCtvLynx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "How to use: replace \"bear\" with the class of the object that Stable Diffusion is bring trained to generated.\n",
        "'''\n",
        "class_name = \"bear\"\n",
        "find_tokens(class_name, 10)"
      ],
      "metadata": {
        "id": "yedvN4JTLzT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "How to use: replace arguments with the two categories. If they are the same, category, put the same word twice.\n",
        "'''\n",
        "\n",
        "find_tokens_two_categories(\"cat\", \"dog\", 10)"
      ],
      "metadata": {
        "id": "laYSLsuKMDDC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}